\documentclass[es,practica]{uah}

\usepackage{hyperref}

\tema{1}
\titulo{Teoría de la Información. Codificación de fuente}{Lesson title}
%
\begin{document}

\titulacion{Optativa GIEC y GIT}
\departamento{Teoría de la Señal y Comunicaciones}
\asignatura{Comunicaciones Digitales}{}
\curso{2021/2022}

\maketitle

\begin{abstract}
Comenzaremos esta práctica repasando el concepto de entropía, para a continuación ver dos ejemplos de codificadores de fuente, como son los códigos Huffman y los códigos LZW (Lempel-Ziv-Welch)
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%
\section{Introducción}
%%%%%%%%%%%%%%%%%%%%%%

En el directorio de Python hay varios ficheros que utilizaremos para esta práctica:

\begin{itemize}
	\item \texttt{Practica1a.py}, \texttt{Practica1b.py} y \texttt{Practica1c.py}: Se trata de los archivos principales que debemos ejecutar para desarrollar la práctica. Se refieren, respectivamente, a ejemplos de codificación fuente con un archivo de texto, uno de audio y una imagen.
	\item \texttt{CodFuente.py}. Archivo con una serie de funciones relacionadas con la codificación fuente:
	\begin{itemize}
		\item {\bf huffman}
		\item {\bf huffman\_cod}
		\item {\bf huffman\_dec}
		\item {\bf lzw\_cod}
		\item {\bf lzw\_dec}
	\end{itemize}
\end{itemize}


Dentro del archivo \emph{FuncionesP1.py} veréis que faltan por desarrollar algunas funciones que se han dejado en blanco:
\begin{itemize}
	\item {\bf gen\_huffman\_dict}: Esta función debe tomar a su entrada un mensaje en formato \emph{string}, \emph{List} ó \emph{ndarray} (array de NumPy), en función del tipo de archivo con el que estemos trabajando, y devolver a su salida un diccionario con pares (símbolo, probabilidad). Algunas sugerencias para esta función:
	\begin{itemize}
		\item \texttt{np.ravel(a)}: Esta función convierte el array \emph{a} de más de una dimensión (e.g. una imagen) en un array unidimensional.
		\item \texttt{list(x)}: Crea una lista a partir de la variable que le demos. Por ejemplo, si es una cadena, genera una lista con cada carácter de la cadena por separado.
		\item \texttt{collections.Counter(x)}: A partir de una lista, devuelve el recuento del número de ocurrencias de cada elemento de la lista. 
		\item \texttt{dict(x)}: Crea un diccionario a partir de un contador como el del punto anterior. 
	\end{itemize}

	\item {\bf gen\_lzw\_dict}: Función muy similar a la anterior, pero esta vez la salida es un diccionario con pares (símbolo, código), donde el código es simplemente un número entero que enumera cada uno de los posibles elementos de la entrada.

	\item {\bf entropia}: Esta función debe tomar a su entrada un mensaje en formato \emph{string} ó \emph{List}, y devolver a su salida el valor de la entropía para dicha fuente. Podéis aprovecharos de la función \emph{gen\_huffman\_dict} para coger de ahí las probabilidades de cada uno de los símbolos.
	\item {\bf tamanoOriginal}: Devuelve el tamaño, en KB, de un mensaje de entrada al sistema. Toma como entradas el propio mensaje y el número de bits por cada símbolo del mensaje, que serán 8 para el caso de iamgen o texto y 16 para audio. 
	\item {\bf tamanoMinimo}: Devuelve el tamaño mínimo, en KB, que se podría conseguir si alcanzásemos una eficiencia de codificación máxima.
	\item {\bf tamanoCodificado}: Devuelve el tamaño, en KB, de una secuencia de bits dada. 
\end{itemize}

Se puede comprobar el funcionamiento de estas funciones utilizando la cadena de datos ``una prueba''. Se debería obtener una entropía de $2.92$ y los siguientes diccionarios:
\begin{itemize}
	\item Huffman: \begin{verbatim}{'u': 0.2, 'n': 0.1, 'a': 0.2, ' ': 0.1, 'p': 0.1, 'r': 0.1, 'e': 0.1, 'b': 0.1}\end{verbatim}
	\item LZW: \begin{verbatim}{'b': 7, 'e': 6, 'r': 5, 'p': 4, ' ': 3, 'a': 2, 'n': 1, 'u': 0}\end{verbatim}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Archivo de texto}
%%%%%%%%%%%%%%%%%%%%%%%%%%
Una vez hayáis completado estas funciones ya podéis abrir el fichero \texttt{Practica1a.py} y ejecutarlo. Comprobad que el fichero de salida (salida.txt) coincida con la entrada, y los valores que obtenéis de los tamaños de los archivos en cada fase tanto para el caso de utilizar codificación Huffman como codificación LZW.


%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Archivo de audio}
%%%%%%%%%%%%%%%%%%%%%%%%%%
Para el caso de archivos de audio (y de imagen, como veremos más adelante), es posible utilizar algunas estrategias ligeramente distintas a las que hemos visto. En estos casos ese habla de codificación con pérdidas (lo que hacen los sistemas mp3, aac, mp4 y similares), ya que vamos a reducir el tamaño del archivo, al igual que hemos hecho antes, pero en este caso vamos a asumir que el archivo resultante no sea igual que el original, aunque sí lo suficientemente parecido como para que las diferencias no sean perceptibles. Este tipo de codificadores se denominan a veces ''codificadores de destino`` ya que basan gran parte de su funcionamiento en las características del receptor (en el caso del audio, el oído humano).

La idea es muy simple. Previamente a la codificación Huffman, lo que se hace es aplicar un cuantificador al archivo original, con la idea de reducir el número de bits por muestra de los 16 que suelen ser normales en los ficheros de audio, a un valor mucho menor. Como sabemos, el hecho de realizar una cuantificación va a implicar necesariamente que se introduzca un determinado ruido de cuantificación (cuantos menos bits utilicemos por muestra, mayor será el ruido de cuantificación). Un codificador de audio lo que va a hacer es intentar controlar este ruido de cuantificación de modo que quede enmascarado por el sonido original, de forma que no sea audible. Si se hace correctamente, el archivo codificado y el original serían prácticamente indistinguibles desde un punto de vista perceptual. 

Nosotros vamos a hacer algo mucho más sencillo, pero que permite observar el funcionamiento de un sistema de este tipo. En el archivo \texttt{Practica1b.py} se realiza la codificación de un archivo de audio de dos formas distintas:

\begin{itemize}
	\item {\bf En el dominio del tiempo}: Esto no es lo habitual (de hecho nunca se hace así), pero como primera aproximación lo que hacemos es simplemente recuantificar el archivo original, aplicarle un codificador Huffman y deshacer todo el proceso en recepción. Podéis observar el resultado final en el archivo \emph{salida1.wav}.
	\item {\bf En el dominio de la frecuencia}: En este caso lo que se hace es dividir el fichero de audio en tramas de N muestras, y para cada una de ellas se calcula una transformada al dominio de la frecuencia (usamos una transformada del coseno, DCT, en lugar de una transformada de Fourier porque devuelve valores reales, lo que facilita el trabajo). Una vez en el dominio de la frecuencia, se recuantifica cada una de las tramas, se le aplica un codificador Huffman, y el receptor deshace todo el proceso. La razón de trabajar de esta manera es que el proceso de enmascaramiento del oído humano también funciona en el domino de la frecuencia, por lo que al trabajar en este dominio podemos ajustar mucho mejor el funcionamiento del codificador a las características del sonido. En este caso podéis ver la salida del codificador en el archivo \emph{salida2.wav}.
\end{itemize}


En realidad el proceso es algo más complejo. Si queréis profundizar algo más podéis echarle un ojo a este tutorial para haceros una idea: \url{https://ieeexplore.ieee.org/document/618009}. Podéis acceder a él desde la intranet de la UAH o utilizando la VPN. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Archivo de imagen}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

Este apartado tiene como idea complementar lo ya hecho anteriormente con un ejemplo de aplicación real como es la codificación de imágenes. En concreto vamos a pensar en una versión muy simplificada de un codificador JPEG que es lo que podéis encontrar en el archivo \texttt{Practica1c.py}.

JPEG se enmarca dentro de lo que se conocen como codificadores con pérdidas. Esto quiere decir que la imagen original nunca se va a poder volver a reconstruir a partir de la imagen en formato JPEG, ya que hay partes de la información que se van a eliminar. Igual que antes, esta información se elimina de forma que su impacto visual sobre la imagen sea el menor posible, de forma que, si todo va bien, la imagen JPEG y la imagen original sean prácticamente idénticas. 

Para conseguir todo esto, el algoritmo JPEG hace lo siguiente con la imagen (vamos a pensar por simplicidad que tenemos una imagen en escala de grises):

\begin{enumerate}
\item Antes de nada, convertimos los valores de cada pixel de la imagen de formato \emph{entero sin signo} a \emph{entero con signo}. Para un caso como el que vamos a considerar, en el que la imagen viene codificada con 8 bits por muestra, esto implica simplemente restarle $2^7 = 128$ a cada muestra, de forma que los valores pasan de $[0, 255]$ a $[-128, 127]$.
\item Ahora procesamos la imagen en bloques de 8x8 píxeles.
\item Se calcula la Transformada del Coseno (DCT) para cada bloque. Como comentábamos antes, la DCT es una variante de la transformada de Fourier en la que las funciones base en lugar de ser exponenciales complejas son cosenos. Esto tiene algunas limitaciones que no vienen al caso, pero también tiene una gran ventaja, y es que los valores de la DCT son reales, por lo que reducimos a la mitad la información a codificar con respecto a una DFT normal.
\item Se cuantifica el bloque transformado utilizando una matriz de cuantificación. El estándar propone una, aunque luego cada fabricante puede utilizar una propia. Si llamamos $Q$ a la matriz de cuantificación y $X$ al bloque obtenido tras la DCT, el proceso es simplemente:
\begin{equation}
	X_q = Round \left ( \frac{X}{Q} \right )
\end{equation}
\item Se aplica un proceso de codificación Huffman a la señal cuantificada
\end{enumerate}

Una vez en el decodificador, se siguen los pasos inversos para cada bloque:

\begin{enumerate}
\item Hacemos la cuantificación inversa:
\begin{equation}
	X_R = X_q \cdot Q
\end{equation}
\item Calculamos la transformada inversa del coseno (IDCT).
\end{enumerate}


En realidad lo que se hace es que se codifica de forma separada el primer pixel de cada bloque de 8x8 (que tiene un valor muy superior al resto de píxeles del bloque)\footnote{Si accedéis desde la UAH, o con VPN, podéis echarle un ojo a este tutorial para haceros una idea: \url{https://ieeexplore.ieee.org/document/125072}}. El valor de estos píxeles iniciales de cada bloque se codifica de forma diferencial con el del anterior bloque, ahorrando así algún bit extra, y el resto de los píxeles sí que se codifican utilizando un código Huffman, ordenando los valores siguiendo un patron en diagonal.



\section{Qué hay que entregar}

\begin{itemize}
	\item El archivo \texttt{FuncionesP1.py}.
	\item Un documento de texto en el que se responda a lo siguiente:
	\begin{itemize}
		\item ¿Qué diferencias aprecias entre la codificación Huffman y la LZW para el archivo de texto?
		\item Para el caso del archivo de audio, ¿qué diferencia existe entre trabajar en el dominio del tiempo o en el dominio transformado? ¿Qué sucede si aumentas o disminuyes el valor del escalón de cuantificación (está en la parte de Configuración del archivo \texttt{Practica1b.py})?
		\item Lo mismo para el caso de la imagen. ¿Cómo afecta el valor del factor de calidad a los resultados?
	\end{itemize}
\end{itemize}

% En primer lugar debemos desarrollar las siguientes funciones en Matlab:

% \begin{enumerate}
% 	\item Generar una función que, dado un vector de probabilidades $p$ de una determinada fuente de información, devuelva el valor de la entropía para esa fuente.\\
% 		\texttt{function H = entropia(p)}
% 	\item Generar una función que, a partir de un vector de probabilidades p de una determinada fuente de información, calcule el código Huffman correspondiente.\\
% 		\texttt{function codigo = huffman(p)}\\
% 	Podéis comprobar el funcionamiento de las funciones creadas considerando una fuente con 6 símbolos posibles con probabilidades:
% 		\texttt{p = [0,1, 0,3, 0,05, 0,09, 0,21, 0,25]}
% 		La entropía de esta fuente es de $2,3549$ bits por símbolo de fuente, la longitud media de las palabras del código Huffman es de $2,38$ bits, y la eficiencia $0,9895$.
% 	\item Generar una función que codifique un determinado mensaje utilizando el algoritmo LZW utilizando un diccionario inicial dado.\\
% 		\texttt{function codigo = lzw(cadena, diccionario)}
% 	\item Generar la función que sea capaz de decodificar un mensaje codificado con LZW tomando como dato el diccionario inicial.\\
% 		\texttt{function mensaje = lzwdec(codigo, diccionario)}
% 		Para probar el algoritmo de codificación LZW, podéis utilizar un diccionario que conste únicamente de dos símbolos: $diccionario = {'A','B'}$, y codificar la palabra $ABAABABA$. El resultado deberían de ser 6 códigos:\\
% 		\texttt{codigo = [1 2 1 3 6 ]}
% 	Vamos a considerar a partir de ahora un diccionario consistente en todas las letras del español en minúscula, con sus correspondientes probabilidades de ocurrencia, que se suministran en las variables diccionario y $p$ del archivo Datos.mat:	
	
% 	\begin{center}
% 	\begin{tabular}{l|l}
% 	Carácter & Probabilidad \\
% 	\hline
% 	ESPACIO & $0.1899$\\
% 	a & $0.0934$\\
% 	b & $0.0179$\\
% 	c & $0.0326$\\
% 	d & $0.0406$\\
% 	\hline
% 	e & $0.0987$\\
% 	f & $0.0056$\\
% 	g & $0.0143$\\
% 	h & $0.0057$\\
% 	i & $0.0506$\\
% 	\hline
% 	j & $0.0040$\\
% 	k & $0.0001$\\
% 	l & $0.0402$\\
% 	m & $0.0256$\\
% 	n & $0.0544$\\
% 	\hline
% 	o & $0.0703$\\
% 	p & $0.0203$\\
% 	q & $0.0071$\\
% 	r & $0.0557$\\
% 	s & $0.0646$\\
% 	\hline
% 	t & $0.0375$\\
% 	u & $0.0237$\\
% 	v & $0.0092$\\
% 	w & $0.0001$\\
% 	x & $0.0017$\\
% 	\hline
% 	y & $0.0082$\\
% 	z & $0.0038$\\
% 	á & $0.0041$\\
% 	é & $0.0035$\\
% 	í & $0.0059$\\
% 	\hline
% 	ó & $0.0067$\\
% 	ú & $0.0014$\\
% 	ü & $0.0001$\\
% 	ñ & $0.0025$\\	
% 	\end{tabular}
% \end{center}


% \item Una vez que tenemos listo todo lo anterior, vamos a realizar las siguientes pruebas:

% \begin{itemize}
% 	\item Considerar una fuente con probabilidades de símbolo:
% 	\begin{displaymath}
% 		p = \left [ \frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{16}, \frac{1}{32}, \frac{1}{64}, \frac{1}{128}, \frac{1}{256}, \frac{1}{256} \right ]
% 	\end{displaymath}
% Calcular la eficiencia de este código. ¿Qué debe ocurrir para que un código tenga eficiencia 1?

% 	\item Calcular la entropía del alfabeto español con los datos dados.
% 	\item Obtener el código Huffman correspondiente a las letras del español, calculando la longitud media de palabra, la tasa de compresión y la eficiencia.
% 	\item Hacer lo mismo que en el apartado anterior, pero suponiendo ahora que las letras se van a agrupar de dos en dos. Supondremos que la fuente de información no tiene memoria, de forma que la probabilidad de aparición de un par de letras es igual al producto de las probabilidades individuales.
% 	\item Codificar, utilizando el algoritmo LZW, la cadena de texto mensaje incluida en el archivo \emph{Datos.mat}. Obtener, como antes, la longitud media de palabra, la tasa de compresión y la eficiencia obtenidas.
% 	\item Decodificar el mensaje decodificado, comprobando que se obtiene de nuevo el mensaje original.
% \end{itemize}



% \section{Pistas}
% \begin{itemize}
% 	\item Para la codificación Huffman, es muy cómodo trabajar con números enteros, y en el último momento traducirlos a binario con la función \texttt{dec2bin}. Añadir un 0 a la derecha de un número binario no es más que multiplicarlo por dos en decimal, y añadir un 1, multiplicar por 2 y sumar 1.
% 	\item El diccionario del codificador LZW os lo paso en formato de \emph{cell array} porque creo que es lo más cómodo, ya que cada entrada del diccionario puede tener una longitud distinta.
% 	\item El comando \texttt{strcmp(cadena,diccionario)} resulta muy útil para localizar si una determinada cadena existe en el diccionario, y dónde se encuentra.
% 	\item Para el caso de la codificación del texto por pares de caracteres es útil tener en cuenta la función \texttt{kron}.
% \end{itemize}

% \section{¿Qué entregar?}
% \begin{itemize}
% 	\item Código de las funciones generadas
% \end{itemize}


% \end{enumerate}

\end{document}



	
